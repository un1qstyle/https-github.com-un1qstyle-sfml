{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gavrilov_ej\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Word2Ve'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bd429dbab538>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDBSCAN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Импортируем библиотеки Word2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2vec\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWord2Ve\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m \u001b[1;31m# Вектора.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Word2Ve'"
     ]
    }
   ],
   "source": [
    "#импорт библиотек\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_log_error as MSLE\n",
    "import pymorphy2 # Морфологический анализатор.\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import umap\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "# Импортируем библиотеки Word2Vec\n",
    "from gensim.models.word2vec import Word2Ve\n",
    "import numpy as np # Вектора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Начальные константы\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,5)\n",
    "morph=pymorphy2.MorphAnalyzer() # Создает объект морфоанализатора и загружет словарь.\n",
    "\n",
    "posConv={'ADJF':'_ADJ','NOUN':'_NOUN','VERB':'_VERB'}\n",
    "#meaningfullPoSes=['ADJF', 'NOUN', 'VERB']\n",
    "meaningfullPoSes=['NOUN']\n",
    "meaningfullAdj=['ADJF']\n",
    "#model_w2v = KeyedVectors.load_word2vec_format('data/rusvec.vec.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Для удаления полей, которые я не буду использовать\n",
    "def DeleteUnnesField(DF):\n",
    "    new_DF = DF.drop(columns=['contacts_visible', 'date_created', 'description',\\\n",
    "                              'fields', 'images', 'location', 'subway'], axis = 1)\n",
    "    return new_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetNameString(NameString):\n",
    "    \n",
    "    name   = str(NameString);\n",
    "    words      = name.split();\n",
    "    resultlist = []\n",
    "\n",
    "    for w in words:\n",
    "        \n",
    "        if len(w) < 2:\n",
    "            continue\n",
    "        \n",
    "        wordform = morph.parse(w)[0]\n",
    "        resultlist.append(wordform.normal_form)\n",
    "    \n",
    "    return resultlist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Для определения субкласса в дополнение к категории и субкатегории по \n",
    "#первому существительному и прилагательному\n",
    "def GetVecByName(col_name):\n",
    "    \n",
    "    Vec        = np.zeros(shape=(100))  \n",
    "    count      = 0\n",
    "    \n",
    "    for word in set(col_name):\n",
    "        try:\n",
    "            Vec+= modelw2v[word]\n",
    "            count+=1\n",
    "        except:\n",
    "            count=count\n",
    "            \n",
    "    return np.dot(Vec, np.ones(100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#не используется\n",
    "#Для определения субкласса в дополнение к категории и субкатегории по \n",
    "#первому существительному и прилагательному\n",
    "def GetClassByName(col_name):\n",
    "    \n",
    "    name   = str(col_name);\n",
    "    words      = name.split();\n",
    "    resultlist = []\n",
    "\n",
    "    for w in words:\n",
    "        wordform = morph.parse(w)[0]\n",
    "        if wordform.tag.POS in meaningfullPoSes:\n",
    "            resultlist.append(wordform.normal_form+posConv[wordform.tag.POS])\n",
    "            #resultlist.append(wordform.normal_form)\n",
    "            break\n",
    "    \n",
    "    for w in words:\n",
    "        wordform = morph.parse(w)[0]\n",
    "        if wordform.tag.POS in meaningfullAdj:\n",
    "            resultlist.append(wordform.normal_form+posConv[wordform.tag.POS])\n",
    "            #resultlist.append(wordform.normal_form)\n",
    "            break\n",
    "\n",
    "    Vec        = np.zeros(shape=(600))  \n",
    "    count      = 0\n",
    "    \n",
    "    for word in set(resultlist):\n",
    "        try:\n",
    "            Vec+= model_w2v[word]\n",
    "            count+=1\n",
    "        except:\n",
    "            #print(word)\n",
    "            count=count\n",
    "    \n",
    "    if count > 0:\n",
    "        Vec = Vec/count\n",
    "\n",
    "    return np.dot(Vec, np.ones(600))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Не используется. Изначально хотел по нормализованным словам получить класс,\n",
    "#но различие тестовой и тренировочной выборки не позволило\n",
    "def GetSubclasses(DF):\n",
    "    category = set(DF.category)\n",
    "    subcateg = set(DF.subcategory)\n",
    "    resultDF = DF[DF.category==-1]\n",
    "\n",
    "    for i in category:\n",
    "        for j in subcateg:\n",
    "            df = DF[(DF.category==i) & (DF.subcategory==j)]\n",
    "        \n",
    "            if len(df)==0:\n",
    "                 continue\n",
    "                    \n",
    "            WordSet = set(df.norm_name)    \n",
    "            \n",
    "            #составление словаря\n",
    "            WordDict = {}\n",
    "            count = 0\n",
    "            for item in WordSet:\n",
    "                WordDict.update({item: count})\n",
    "                count+=1\n",
    "\n",
    "            #Получение подкласса\n",
    "            df.loc[:,'subclass'] = df.loc[:,'norm_name'].apply(lambda x: WordDict.get(x))\n",
    "   \n",
    "            resultDF=pd.concat([resultDF, df])\n",
    "            \n",
    "            #break\n",
    "        #break\n",
    "    return resultDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Читаем тренировочную выборку \n",
    "with open('data/train_hack.pckl','rb') as f:\n",
    "    data_new = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Читаем тестовую выборку\n",
    "with open('data/test_hack.pckl','rb') as f:\n",
    "    data_test = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#удаляем столбцы\n",
    "train_norm = DeleteUnnesField(data_new)\n",
    "test_norm = DeleteUnnesField(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Освобождаю память\n",
    "data_new = 1\n",
    "data_test = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Получение представления наименования в виде числа\n",
    "train_norm['det'] = train_norm['name'].map(GetClassByName)\n",
    "test_norm['det'] = test_norm['name'].map(GetClassByName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Получение \"нормальных\" имен\n",
    "train_norm['norm_name'] = train_norm['name'].map(GetNameString)\n",
    "test_norm['norm_name'] = test_norm['name'].map(GetNameString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentences = train_norm['norm_name'].tolist() + test_norm['norm_name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Обучение модели для имен\n",
    "modelw2v = Word2Vec(sentences=Sentences,seed=127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gavrilov_ej\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#Получение \"субкласса\" товара\n",
    "train_norm['det'] = train_norm['norm_name'].map(GetVecByName)\n",
    "test_norm['det'] = test_norm['norm_name'].map(GetVecByName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#трейн\n",
    "X = train_norm.drop(columns=['id','name','price','norm_name'])\n",
    "y = train_norm.price/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#фит\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "#model = RandomForestRegressor()\n",
    "\n",
    "model = Pipeline([\n",
    "    ('onehot', OneHotEncoder(categorical_features=[2,6])),\n",
    "    ('reg', RandomForestRegressor(random_state=127, n_estimators=20))\n",
    "])\n",
    "\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X)\n",
    "MSLE(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задаем значения параметров, которые хотим проверить\n",
    "# Теперь с распределениями\n",
    "param_grid = {\n",
    "    'reg__n_estimators': range(10,40,5),\n",
    "    'reg__criterion': ['mse','mae'],\n",
    "    'reg__max_depth': range(2, 8),\n",
    "    'reg__mean_samples_leaf':range(1,100)\n",
    "}\n",
    "\n",
    "# Задаем схему кросс-валидации\n",
    "cv = StratifiedKFold(n_splits=5, random_state=127, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Рассмотрим 20 случайных комбинаций\n",
    "#random_searcher = Ran(model, param_grid, n_iter=10, \n",
    "#                                     random_state=127,\n",
    "#                                      \n",
    "#                                     n_jobs=-1, cv=cv, \n",
    "#                                     verbose=2,error_score=0)\n",
    "\n",
    "#random_searcher.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_norn.drop(columns=['id','name','norm_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = model.predict(X_test)\n",
    "#MSLE(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesub = pd.read_csv('data/submit_Sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_norn['price']=y_test_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Хоть какой-то результат для kaggle\n",
    "test_norn.to_csv('predict.csv', index=None,columns=['id','price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_this = train_norm[(train_norm.category==22) & (train_norm.subcategory==2204)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "curmodel = Word2Vec(sentences=on_this.norm_name.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gavrilov_ej\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\gavrilov_ej\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "on_this['det'] = on_this['norm_name'].map(lambda x: GetVecByName1(x, curmodel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = on_this.drop(columns=['id','name','price','norm_name','predict'])\n",
    "y = on_this.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('onehot', OneHotEncoder(categorical_features=[2, 6], dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)), ('reg', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           mi...stimators=20, n_jobs=1,\n",
       "           oob_score=False, random_state=127, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#фит\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "#model = RandomForestRegressor()\n",
    "\n",
    "model2 = Pipeline([\n",
    "    ('onehot', OneHotEncoder(categorical_features=[2,6])),\n",
    "    ('reg', RandomForestRegressor(random_state=127, n_estimators=20))\n",
    "])\n",
    "\n",
    "model2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.513834408724913"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = model.predict(x_test)\n",
    "MSLE(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
